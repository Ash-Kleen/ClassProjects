{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40caaf9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This dataset comes from the NASA program SLAP(Scanning L-Band Active Passive). Their main goal is to measure soil moisture using \n",
    "temperature readings. These readings are taken using a device that is attached to a plane which flies in a rectangular pattern to capture a pre-determined plot of earth. The device used to take readings utilizes an oscialliting sensor that takes readings in an eliptical pattern. \n",
    "\n",
    "There is a need for this group to accurately determine which areas are land and which are water. This is mainly found through temperature readings, and the algorithms they use are inaccurate due to the fact they are mainly using google earth and guessing by eye (Husband has verified this to be true). \n",
    "\n",
    "For this project, I will be using KMeans to assign new flags for water. My goal is to identify at least 3 values (water, shoreline, and dry land).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b0e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import folium\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362e5fa",
   "metadata": {},
   "source": [
    "# Data\n",
    "Dataset link: https://earth.gsfc.nasa.gov/hydro/instruments/slap/campaigns/slapex-liaise-campaign-spain-2021\n",
    "\n",
    "The dataset I am using is from a campaign in 2021 called \"Land surface Interactions with the Atmosphere over the Iberian Semi-arid Environment\" (LIAISE). This LIASE campaign had a total of 9 flights with a mix of low and medium altitudes. \n",
    "\n",
    "I chose the 17 July 2021, low altitude cleaned version (if you look in the L2_SM_P column). The SLAP group cleans the data for radio interference and adds flags for water, among other things. \n",
    "\n",
    "This dataset has a shape of 320855, 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9803a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/ashkl/OneDrive/Documents/Loyola/machine learning/SLAP.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109a6779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320855, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebe1d6",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "I will be removing an NA values. The first portion of the data is actually a set of calibrations that are done on land. If you see the below, there are 137,852 NA values for footprint_lat and footprint_lon, so I will remove those, then there was still a few hiding in the NEDT_v column, so I removed them as well. I will also need to ensure there are no blank, hidden spaces in the column names. After NA removal, I am left with 182,419 rows which I believe will be plenty to achieve my goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3f9862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " TBh                             74976\n",
       " TBv                             74958\n",
       " TB_IR                               0\n",
       " NEDT_h                          74976\n",
       " NEDT_v                          74958\n",
       " footprint_lat                  137852\n",
       " footprint_lon                  137852\n",
       " footprint_semimajor             88431\n",
       " footprint_semiminor             88431\n",
       " footprint_semimajor_azimuth    137852\n",
       " aircraft_lat                        0\n",
       " aircraft_lon                        0\n",
       " aircraft_alt_MSL                    0\n",
       " aircraft_alt_AGL                    0\n",
       " aircraft_roll_angle             46163\n",
       " aircraft_pitch_angle            46163\n",
       " aircraft_heading                46803\n",
       " flag_RFI_time_domain_h              0\n",
       " flag_RFI_time_domain_v              0\n",
       " flag_RFI_cross_frequency_h          0\n",
       " flag_RFI_cross_frequency_v          0\n",
       " flag_RFI_kurtosis_h                 0\n",
       " flag_RFI_kurtosis_v                 0\n",
       " flag_contam_h                       0\n",
       " flag_contam_v                       0\n",
       " flag_flight                         0\n",
       " flag_box_calibration                0\n",
       " flag_water_calibration              0\n",
       " flag_temperature_stable_h           0\n",
       " flag_temperature_stable_v           0\n",
       " flag_antenna_rotate                 0\n",
       " flag_radar                          0\n",
       " flag_iridium                        0\n",
       " flag_roll_angle                     0\n",
       " flag_GLORI_box                      0\n",
       " flag_VAS_site                       0\n",
       " flag_lake                           0\n",
       " flag_water                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd2aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBh                            0\n",
      "TBv                            0\n",
      "TB_IR                          0\n",
      "NEDT_h                         0\n",
      "NEDT_v                         0\n",
      "footprint_lat                  0\n",
      "footprint_lon                  0\n",
      "footprint_semimajor            0\n",
      "footprint_semiminor            0\n",
      "footprint_semimajor_azimuth    0\n",
      "aircraft_lat                   0\n",
      "aircraft_lon                   0\n",
      "aircraft_alt_MSL               0\n",
      "aircraft_alt_AGL               0\n",
      "aircraft_roll_angle            0\n",
      "aircraft_pitch_angle           0\n",
      "aircraft_heading               0\n",
      "flag_RFI_time_domain_h         0\n",
      "flag_RFI_time_domain_v         0\n",
      "flag_RFI_cross_frequency_h     0\n",
      "flag_RFI_cross_frequency_v     0\n",
      "flag_RFI_kurtosis_h            0\n",
      "flag_RFI_kurtosis_v            0\n",
      "flag_contam_h                  0\n",
      "flag_contam_v                  0\n",
      "flag_flight                    0\n",
      "flag_box_calibration           0\n",
      "flag_water_calibration         0\n",
      "flag_temperature_stable_h      0\n",
      "flag_temperature_stable_v      0\n",
      "flag_antenna_rotate            0\n",
      "flag_radar                     0\n",
      "flag_iridium                   0\n",
      "flag_roll_angle                0\n",
      "flag_GLORI_box                 0\n",
      "flag_VAS_site                  0\n",
      "flag_lake                      0\n",
      "flag_water                     0\n",
      "dtype: int64 (182419, 38)\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.replace(' ', '')\n",
    "df = df[df['footprint_lat'].notna() & df['NEDT_v'].notna()]\n",
    "print(df.isna().sum(), df.shape)\n",
    "#TBh and TBv are horizontal and vertical polarization. They are from 2 diff mechanisms and take slightly diff readings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ebfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['TBh', 'TBv', 'TB_IR', 'footprint_lat', 'footprint_lon', 'footprint_semimajor', 'footprint_semiminor', \n",
    "           'footprint_semimajor_azimuth', 'aircraft_alt_MSL', 'aircraft_alt_AGL', 'flag_RFI_time_domain_h', 'flag_RFI_time_domain_v',\n",
    "          'flag_temperature_stable_h', 'flag_temperature_stable_v', 'flag_water', 'flag_contam_h', 'flag_contam_v']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe256b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TBh</th>\n",
       "      <th>TBv</th>\n",
       "      <th>TB_IR</th>\n",
       "      <th>footprint_lat</th>\n",
       "      <th>footprint_lon</th>\n",
       "      <th>footprint_semimajor</th>\n",
       "      <th>footprint_semiminor</th>\n",
       "      <th>footprint_semimajor_azimuth</th>\n",
       "      <th>aircraft_alt_MSL</th>\n",
       "      <th>aircraft_alt_AGL</th>\n",
       "      <th>flag_RFI_time_domain_h</th>\n",
       "      <th>flag_RFI_time_domain_v</th>\n",
       "      <th>flag_temperature_stable_h</th>\n",
       "      <th>flag_temperature_stable_v</th>\n",
       "      <th>flag_water</th>\n",
       "      <th>flag_contam_h</th>\n",
       "      <th>flag_contam_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59309</th>\n",
       "      <td>262.38</td>\n",
       "      <td>284.83</td>\n",
       "      <td>294.27</td>\n",
       "      <td>39.531</td>\n",
       "      <td>-0.41914</td>\n",
       "      <td>70.323</td>\n",
       "      <td>33.307</td>\n",
       "      <td>30.001</td>\n",
       "      <td>308.39</td>\n",
       "      <td>253.39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59310</th>\n",
       "      <td>263.18</td>\n",
       "      <td>285.42</td>\n",
       "      <td>294.93</td>\n",
       "      <td>39.531</td>\n",
       "      <td>-0.41914</td>\n",
       "      <td>70.333</td>\n",
       "      <td>33.312</td>\n",
       "      <td>28.944</td>\n",
       "      <td>308.43</td>\n",
       "      <td>253.43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59311</th>\n",
       "      <td>271.41</td>\n",
       "      <td>287.66</td>\n",
       "      <td>294.78</td>\n",
       "      <td>39.531</td>\n",
       "      <td>-0.41914</td>\n",
       "      <td>70.344</td>\n",
       "      <td>33.318</td>\n",
       "      <td>27.757</td>\n",
       "      <td>308.47</td>\n",
       "      <td>253.47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59312</th>\n",
       "      <td>266.14</td>\n",
       "      <td>295.15</td>\n",
       "      <td>294.93</td>\n",
       "      <td>39.531</td>\n",
       "      <td>-0.41914</td>\n",
       "      <td>70.355</td>\n",
       "      <td>33.323</td>\n",
       "      <td>26.469</td>\n",
       "      <td>308.51</td>\n",
       "      <td>253.51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59313</th>\n",
       "      <td>271.18</td>\n",
       "      <td>292.41</td>\n",
       "      <td>294.95</td>\n",
       "      <td>39.531</td>\n",
       "      <td>-0.41916</td>\n",
       "      <td>70.366</td>\n",
       "      <td>33.328</td>\n",
       "      <td>25.048</td>\n",
       "      <td>308.55</td>\n",
       "      <td>253.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TBh     TBv   TB_IR  footprint_lat  footprint_lon  \\\n",
       "59309  262.38  284.83  294.27         39.531       -0.41914   \n",
       "59310  263.18  285.42  294.93         39.531       -0.41914   \n",
       "59311  271.41  287.66  294.78         39.531       -0.41914   \n",
       "59312  266.14  295.15  294.93         39.531       -0.41914   \n",
       "59313  271.18  292.41  294.95         39.531       -0.41916   \n",
       "\n",
       "       footprint_semimajor  footprint_semiminor  footprint_semimajor_azimuth  \\\n",
       "59309               70.323               33.307                       30.001   \n",
       "59310               70.333               33.312                       28.944   \n",
       "59311               70.344               33.318                       27.757   \n",
       "59312               70.355               33.323                       26.469   \n",
       "59313               70.366               33.328                       25.048   \n",
       "\n",
       "       aircraft_alt_MSL  aircraft_alt_AGL  flag_RFI_time_domain_h  \\\n",
       "59309            308.39            253.39                       1   \n",
       "59310            308.43            253.43                       0   \n",
       "59311            308.47            253.47                       0   \n",
       "59312            308.51            253.51                       0   \n",
       "59313            308.55            253.55                       0   \n",
       "\n",
       "       flag_RFI_time_domain_v  flag_temperature_stable_h  \\\n",
       "59309                       1                          1   \n",
       "59310                       1                          1   \n",
       "59311                       1                          1   \n",
       "59312                       1                          1   \n",
       "59313                       0                          1   \n",
       "\n",
       "       flag_temperature_stable_v  flag_water  flag_contam_h  flag_contam_v  \n",
       "59309                          1           0              1              1  \n",
       "59310                          1           0              1              1  \n",
       "59311                          1           0              1              1  \n",
       "59312                          1           0              1              1  \n",
       "59313                          1           0              1              1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff717350",
   "metadata": {},
   "source": [
    "# Feature Engineering and Data Exploration\n",
    "\n",
    "In this section, I am exploring the data and looking at correlation, checking which features might be best to keep, and checking the temperature distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0383a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='rocket_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52fddc",
   "metadata": {},
   "source": [
    "## Checking for best features using SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cc26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['flag_water'], axis=1)\n",
    "y = df.flag_water\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "# Borrowing some code here from homework 8\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=4)\n",
    "fit = bestfeatures.fit(X_train,y_train)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(df.columns)\n",
    "\n",
    "# Since I want to see all of the scores, I changed the printout to include all. \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Feature','Score']\n",
    "print(featureScores.nlargest(17,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep anything with k-score greater than or equal to 2000, then check out the new df\n",
    "df_high_Kscore = df.drop(df.columns[[12, 13, 2, 10, 14, 15, 7, 11, 16]], axis=1)\n",
    "df_high_Kscore.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0ab6b",
   "metadata": {},
   "source": [
    "## Looking at the statistics of the kept columns\n",
    "\n",
    "We can see some pretty intersting stats. There is definitely some skew in a few of the features like the TBh and TBv values I want to use to see if we can make new groups of water flags. In the boxplot, we can see this skewness a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see some stats.\n",
    "df_high_Kscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks = list(df_high_Kscore.columns)\n",
    "p = plt.boxplot(df_high_Kscore, labels = xticks, vert = False)\n",
    "plt.title('Box and Whisker Plots of High-socring Features', fontsize = 14)\n",
    "plt.xlabel(\"Reading Value\", fontsize = 12)\n",
    "plt.ylabel(\"Feature\", fontsize = 12)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee4a60",
   "metadata": {},
   "source": [
    "## Taking a closer look at the TBh and TBv temperature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61187f",
   "metadata": {},
   "source": [
    "Now, we can see clearly that our temperature readings (TBh and TBv) both have some issues with outliers. I have been told this is due to the readings mainly taking place on land, so there is a naturally higher temp with land readings. See the distribution graphs below for a better look. The footprint latitude and longitude, we can ignore for now. The two semiminor and semimajor footprints as well as the aircraft_alt_AGL look good, but the aircraft_alt_MSL has some skew. Let's take a closer look at the temp readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834bea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_high_Kscore, x=\"TBh\", color='lightblue', stat='density')\n",
    "sns.kdeplot(data=df_high_Kscore, x=\"TBh\", color='black')\n",
    "plt.title('Distribution Plot of TBh Reading', fontsize = 14)\n",
    "plt.xlabel(\"TBh Value\", fontsize = 12)\n",
    "plt.ylabel(\"Count\", fontsize = 12);\n",
    "#need to plot just water or just land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a15dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_high_Kscore, x=\"TBv\", color='lightblue', stat='density')\n",
    "sns.kdeplot(data=df_high_Kscore, x=\"TBv\", color='black')\n",
    "plt.title('Distribution Plot of TBv Reading', fontsize = 14)\n",
    "plt.xlabel(\"TBv Value\", fontsize = 12)\n",
    "plt.ylabel(\"Count\", fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e74f85",
   "metadata": {},
   "source": [
    "Based on the distribution plots of the temperature readings, we can see that there are at least 3, possibly 4 modals if we look at the TBh reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe60b",
   "metadata": {},
   "source": [
    "## Map of readings\n",
    "\n",
    "By looking at the map, we can see a picture of where the water/land is and get a good perspective of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ca175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get center of map so we can see the map\n",
    "import statistics\n",
    "print(\"latitude: \", statistics.mean(df[\"footprint_lat\"]))\n",
    "print(\"longitude: \", statistics.mean(df[\"footprint_lon\"]))\n",
    "map_df = df[['footprint_lat', 'footprint_lon']].copy()\n",
    "center_of_map = [41.039595601335385, 0.6624781636930398] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e7951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "my_map = folium.Map(location = center_of_map,\n",
    "                   zoom_start = 7,\n",
    "                   width = '90%',\n",
    "                   height = '100%',\n",
    "                   left = '5%',\n",
    "                   right = '5%',\n",
    "                   top = '0%',\n",
    "                    tiles=\"CartoDB Positron\") \n",
    "for index, row in map_df.iterrows():\n",
    "    folium.Circle(location=[row['footprint_lat'], row['footprint_lon']],\n",
    "                  radius=50,\n",
    "                  color='black',\n",
    "                  fill=True,\n",
    "                  fill_color=\"black\",\n",
    "                  fill_opacity=0.5).add_to(my_map)\n",
    "# my_map.save(path + 'Dots_map_only.html')\n",
    "my_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d094d",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Trying different parameters and sub-sets of data. I have a hunch that only using TBh and TBv will give me the best results, but I will try other possiblities as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc27428",
   "metadata": {},
   "source": [
    "## TBh and TBv only\n",
    "Starting with these two variables since I believe this will give the best results. These are Temperatier Brightness readings collected from a Horizontal-facing sensor and a Vertical-facing sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale temperatures\n",
    "sc = StandardScaler()\n",
    "df_high_Kscore[['T_TBh', 'T_TBv']] = sc.fit_transform(df_high_Kscore[['TBh', 'TBv']])\n",
    "\n",
    "#create array of values\n",
    "X = df_high_Kscore[['T_TBh', 'T_TBv']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0385fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do an elbow plot and determine number of clusters\n",
    "def optimise_k_means(data, max_k):\n",
    "    means=[]\n",
    "    inertias=[]\n",
    "    \n",
    "    for k in range(1, max_k):\n",
    "        kmeans=KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)        \n",
    "        means.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    fig=plt.subplots(figsize = (10, 5))\n",
    "    plt.plot(means, inertias, 'o-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "optimise_k_means(df_high_Kscore[['T_TBh', 'T_TBv']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary run of model\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a df for a map with cluster information\n",
    "map_df = df[['footprint_lat', 'footprint_lon']].copy()\n",
    "map_df['cluster'] = km.labels_\n",
    "\n",
    "map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map of preliminary clusters\n",
    "my_map = folium.Map(location = center_of_map,\n",
    "                   zoom_start = 7,\n",
    "                   width = '90%',\n",
    "                   height = '100%',\n",
    "                   left = '5%',\n",
    "                   right = '5%',\n",
    "                   top = '0%',\n",
    "                    tiles=\"CartoDB Positron\") \n",
    "for index, row in map_df.iterrows():\n",
    "    if row['cluster'] == 0:\n",
    "        color = 'green'\n",
    "    elif row['cluster'] == 1:\n",
    "        color = 'brown'       \n",
    "    else:\n",
    "        color = 'blue' \n",
    "    folium.Circle(location=[row['footprint_lat'], row['footprint_lon']],\n",
    "                  radius=50,\n",
    "                  color=color,\n",
    "                  fill=True,\n",
    "                  fill_color=color,\n",
    "                  fill_opacity=0.5).add_to(my_map)\n",
    "# my_map.save(path + 'Dots_map_preliminary.html')\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385434b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary silhoutte graphic\n",
    "\n",
    "# cluster_labels = np.unique(y_km)\n",
    "# n_clusters = cluster_labels.shape[0]\n",
    "# silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "# y_ax_lower, y_ax_upper = 0, 0\n",
    "# yticks = []\n",
    "# for i, c in enumerate(cluster_labels):\n",
    "#     c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "#     c_silhouette_vals.sort()\n",
    "#     y_ax_upper += len(c_silhouette_vals)\n",
    "#     color = cm.jet(float(i) / n_clusters)\n",
    "#     plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "#              edgecolor='none', color=color)\n",
    "\n",
    "#     yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "#     y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "# silhouette_avg = np.mean(silhouette_vals)\n",
    "# plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "# plt.yticks(yticks, cluster_labels + 1)\n",
    "# plt.ylabel('Cluster')\n",
    "# plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('silhoutte plot 1.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926e19a",
   "metadata": {},
   "source": [
    "IN the above silhoutte graphic, we can see that each cluster has vastly different height, but their lengths are somewhat close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran a gridsearch for kicks. I know it doesn't really work for unsupervised learning, but it does give me some ideas!\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_clusters': [3],  \n",
    "              'init':[\"random\", \"k-means++\"],\n",
    "              'n_init':[\"auto\", 3, 10, 20],\n",
    "              'max_iter':[100, 300, 500],\n",
    "              'algorithm':[\"lloyd\", \"elkan\"],\n",
    "             }  \n",
    "   \n",
    "grid = GridSearchCV(KMeans(), param_grid, refit = True, verbose = False,n_jobs=3) \n",
    "   \n",
    "grid.fit(X) \n",
    " \n",
    "print(grid.best_params_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6903ee1",
   "metadata": {},
   "source": [
    "In the following cell, I run the KMeans with different possible combinations of hyper-parameters. For each, I print out the number of datapoints in each cluster and the silhoutte score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for preliminary test run\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"Preliminary model score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#change init=k-means++\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"Init= k-means++ change score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "\n",
    "#change max iterations to 100\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=100,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"max iteration '100' change score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#change max iteration to 500\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"max iteration '500' change score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#change algorithm\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0,\n",
    "            algorithm='elkan')\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"algorithm 'elkan' change score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffccd7",
   "metadata": {},
   "source": [
    "We can see that for just TBh and TBv, the best model so far is actually a tie between using 100 and 500 iterations with k-means++ and the default algorithm.\n",
    "- Silhoutte score:  0.828039746880101\n",
    "- counts of each cluster:\n",
    "    - 1  ||  142603\n",
    "    - 0  ||   30649\n",
    "    - 2  ||    9167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c4d1d",
   "metadata": {},
   "source": [
    "## TB_IR included with TBh and TBv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac210f6",
   "metadata": {},
   "source": [
    "Next, I include the TB_IR which is the brightness temperature measured by the TIR radiometer in kelvins. This temperature reading is most similar to your every-day infrared thermometer you might use to take your body temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afa3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for all the temperature reading types.\n",
    "temperatures_df = df[['TBh', 'TBv', 'TB_IR']]\n",
    "\n",
    "#transform the values. This is especially needed here due to the different types of measurements being used.\n",
    "temperatures_df[['T_TBh', 'T_TBv', 'T_TB_IR']] = sc.fit_transform(temperatures_df[['TBh', 'TBv', 'TB_IR']])\n",
    "\n",
    "#create the array I need to run KMeans\n",
    "X2 = temperatures_df[['T_TBh', 'T_TBv', 'T_TB_IR']].values\n",
    "\n",
    "#find the number of clusters.\n",
    "optimise_k_means(temperatures_df[['T_TBh', 'T_TBv', 'TB_IR']], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5acb78",
   "metadata": {},
   "source": [
    "Since the number of clusters is a bit ambiguous, I run a ton of tests in the below cell to determine the best number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ed720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "km = KMeans(n_clusters=10, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0,\n",
    "            algorithm='elkan')\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 10 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#9\n",
    "km = KMeans(n_clusters=9, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 9 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#8\n",
    "km = KMeans(n_clusters=8, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0,\n",
    "            algorithm='elkan')\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 8 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#7\n",
    "km = KMeans(n_clusters=7, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 7 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#6\n",
    "km = KMeans(n_clusters=6, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 6 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#5\n",
    "km = KMeans(n_clusters=5, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 5 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#4\n",
    "km = KMeans(n_clusters=4, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 4 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#3\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X2)\n",
    "# print(\"score for 3 clusters: \", silhouette_score(X2, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665136e",
   "metadata": {},
   "source": [
    "None of these did particularly well. The best was using 3 clusters again.\n",
    "- score for 3 clusters:  0.5608960051242237\n",
    "    - 1  ||  86652\n",
    "    - 2  ||  63177\n",
    "    - 0 ||   32590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_map = folium.Map(location = center_of_map,\n",
    "                   zoom_start = 7,\n",
    "                   width = '90%',\n",
    "                   height = '100%',\n",
    "                   left = '5%',\n",
    "                   right = '5%',\n",
    "                   top = '0%',\n",
    "                    tiles=\"CartoDB Positron\") \n",
    "for index, row in map_df.iterrows():\n",
    "    if row['cluster'] == 0:\n",
    "        color = 'green'\n",
    "    elif row['cluster'] == 2:\n",
    "        color = 'brown'       \n",
    "    else:\n",
    "        color = 'blue' \n",
    "    folium.Circle(location=[row['footprint_lat'], row['footprint_lon']],\n",
    "                  radius=50,\n",
    "                  color=color,\n",
    "                  fill=True,\n",
    "                  fill_color=color,\n",
    "                  fill_opacity=0.5).add_to(my_map)\n",
    "my_map.save(path + 'Dots_map_TBIR.html')\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76194a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create silhoutte map on the best of the TB_IR, TBv, and TBh dataset\n",
    "# cluster_labels = np.unique(y_km)\n",
    "# n_clusters = cluster_labels.shape[0]\n",
    "# silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "# y_ax_lower, y_ax_upper = 0, 0\n",
    "# yticks = []\n",
    "# for i, c in enumerate(cluster_labels):\n",
    "#     c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "#     c_silhouette_vals.sort()\n",
    "#     y_ax_upper += len(c_silhouette_vals)\n",
    "#     color = cm.jet(float(i) / n_clusters)\n",
    "#     plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "#              edgecolor='none', color=color)\n",
    "\n",
    "#     yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "#     y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "# silhouette_avg = np.mean(silhouette_vals)\n",
    "# plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "# plt.yticks(yticks, cluster_labels + 1)\n",
    "# plt.ylabel('Cluster')\n",
    "# plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('silhoutte plot 1.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be956b",
   "metadata": {},
   "source": [
    "## TB_IR only\n",
    "\n",
    "I wanted to see if I just used the infrared temperature if it might give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa24d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the array I need to run KMeans. No need for the transformed data since I am only using the one column.\n",
    "X2 = temperatures_df[['TB_IR']].values\n",
    "\n",
    "#find the number of clusters.\n",
    "optimise_k_means(temperatures_df[[ 'TB_IR']], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X3)\n",
    "# print(\"score for 3 clusters: \", silhouette_score(X3, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "\n",
    "#4\n",
    "km = KMeans(n_clusters=4, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X3)\n",
    "# print(\"score for 4 clusters: \", silhouette_score(X3, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#5\n",
    "km = KMeans(n_clusters=5, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X3)\n",
    "\n",
    "# print(\"score for 5 clusters: \", silhouette_score(X3, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since non of the above did very well, may as well try a couple higher numbers\n",
    "#6\n",
    "km = KMeans(n_clusters=6, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X3)\n",
    "# print(\"score for 6 clusters: \", silhouette_score(X3, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "\n",
    "#7\n",
    "km = KMeans(n_clusters=7, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X3)\n",
    "# print(\"score for 7 clusters: \", silhouette_score(X3, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9375f",
   "metadata": {},
   "source": [
    "The best model for only using TB_IR was using 4 clusters. I am not going to bother creating a map for it.\n",
    "- score for 4 clusters:  0.5517634627450231\n",
    "    - 0    62138\n",
    "    - 3    54114\n",
    "    - 1    39146\n",
    "    - 2    27021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b24fdc",
   "metadata": {},
   "source": [
    "## All df_high_Kscore\n",
    "Testing KMeans on the entire dataset of high scoring variables. This was found using SelectKBest at the beginning of this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aee495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform all the variables in df_high_Kscore. Keeping the names the same.\n",
    "df_high_Kscore[['TBh', 'TBv', 'footprint_lon', 'footprint_lat', 'footprint_semiminor', 'footprint_semimajor', 'aircraft_alt_AGL',   \n",
    "               'aircraft_alt_MSL']] = sc.fit_transform(df_high_Kscore[['TBh', 'TBv', 'footprint_lon','footprint_lat', \n",
    "                                                                       'footprint_semiminor','footprint_semimajor',\n",
    "                                                                       'aircraft_alt_AGL','aircraft_alt_MSL']])\n",
    "#create the array needed.\n",
    "X4 = df_high_Kscore.values\n",
    "\n",
    "#find the best number of clusters\n",
    "optimise_k_means(X4, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70935",
   "metadata": {},
   "source": [
    "Almost clearly 3 clusters, but I want to double-check using a couple higher numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X4)\n",
    "# print(\"score for 3 clusters: \", silhouette_score(X4, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "\n",
    "#4\n",
    "km = KMeans(n_clusters=4, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X4)\n",
    "\n",
    "# print(\"score for 4 clusters: \", silhouette_score(X4, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())\n",
    "\n",
    "#5\n",
    "km = KMeans(n_clusters=5, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=500,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X4)\n",
    "\n",
    "# print(\"score for 5 clusters: \", silhouette_score(X4, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96941555",
   "metadata": {},
   "source": [
    "Using 3 clusters wins again! \n",
    "- score for 3 clusters:  0.6839768835574461\n",
    "    - 0    90427\n",
    "    - 2    60989\n",
    "    - 1    31003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba5d41",
   "metadata": {},
   "source": [
    "# Final/Best Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_high_Kscore[['T_TBh', 'T_TBv']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926ea7e",
   "metadata": {},
   "source": [
    "## Best model\n",
    "This model had a silhoutte score of 0.828. Although the thickness varies greatly, I believe this to be expected due to the number of readings on dry land versus wet land or water. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc60ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, \n",
    "            init='k-means++', \n",
    "            n_init=3, \n",
    "            max_iter=100,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(\"max iteration '100' change score: \", silhouette_score(X, y_km))\n",
    "lables = pd.DataFrame(km.labels_)\n",
    "print(lables.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_of_map = 40.296669047966525, 0.346046919653026\n",
    "\n",
    "my_map = folium.Map(location = center_of_map,\n",
    "                   zoom_start = 14,\n",
    "                   width = '90%',\n",
    "                   height = '100%',\n",
    "                   left = '5%',\n",
    "                   right = '5%',\n",
    "                   top = '0%',\n",
    "                    tiles=\"CartoDB Positron\") \n",
    "for index, row in map_df.iterrows():\n",
    "    if row['cluster'] == 2:\n",
    "        color = 'green'\n",
    "    elif row['cluster'] == 1:\n",
    "        color = 'brown'       \n",
    "    else:\n",
    "        color = 'blue' \n",
    "    folium.Circle(location=[row['footprint_lat'], row['footprint_lon']],\n",
    "                  radius=50,\n",
    "                  color=color,\n",
    "                  fill=True,\n",
    "                  fill_color=color,\n",
    "                  fill_opacity=0.5).add_to(my_map)\n",
    "my_map.save(path + 'Dots_map_final.html')\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f82127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_labels = np.unique(y_km)\n",
    "# n_clusters = cluster_labels.shape[0]\n",
    "# silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "# y_ax_lower, y_ax_upper = 0, 0\n",
    "# yticks = []\n",
    "# for i, c in enumerate(cluster_labels):\n",
    "#     c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "#     c_silhouette_vals.sort()\n",
    "#     y_ax_upper += len(c_silhouette_vals)\n",
    "#     color = cm.jet(float(i) / n_clusters)\n",
    "#     plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "#              edgecolor='none', color=color)\n",
    "\n",
    "#     yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "#     y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "# silhouette_avg = np.mean(silhouette_vals)\n",
    "# plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "# plt.yticks(yticks, cluster_labels + 1)\n",
    "# plt.ylabel('Cluster')\n",
    "# plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('silhoutte plot 1.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25430db1",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "I am fairly happy with how KMeans split the clusters. Because it is unsupervised learning, there is no way to determine just how well it' performing. With this dataset, you can map it, but even that does not explain 100%. I do think that we can use it to train a supervised learning model with a bit of work. For instance, you could take a range of lat/lon that you can work with and fine-tune it. \n",
    "\n",
    "It was impressive to see that it was able to pick up on what was clearly water and land. The shoreline, though, might be a little ambiguous since it was picking up areas of warmer water as well as farmlands. For future projects, if the SLAP campaigns travel over forested areas, it would be interesting to see how KMeans might handle it since those areas tend to be a bit cooler than flat, open fields.\n",
    "\n",
    "Other methods to try: DBSCAN, and EM learning.\n",
    "- I did try to use DBSCAN on the dataset, but ran into memory issues, and I felt splitting the data was not the best method.\n",
    "- For EM, I found no easy way to use it in Python, but I feel it might be able to provide a better model since it uses statitical methods versus euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a58412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
